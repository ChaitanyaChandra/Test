### Case1

> Q1

* Create a Pod `mc-pod` in the `mc-namespace` namespace with three containers. 
* The first container should be named `mc-pod-1`, run the `nginx:1-alpine` image, and set an environment variable `NODE_NAME` to the node name. 
* The second container should be named `mc-pod-2`, run the `busybox:1 image`, and continuously log the output of the date command to the file /var/log/shared/date.log every second. 
* The third container should have the name `mc-pod-3`, run the image `busybox:1`, and print the contents of the `date.log` file generated by the second container to stdout. 
* Use a shared, non-persistent volume.

> Acceptance
* Is the pod environment variable correctly set?
* Does the Sidecar display logs?

> Sol

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mc-pod
  namespace: mc-namespace
  labels:
    run: mc-pod
spec:
  containers:
  - name: mc-pod-1
    image: nginx:1-alpine
    env:
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName

  - name: mc-pod-2
    image: busybox:1
    command: ["sh", "-c"]
    args:
      - while true; do
          echo "$(date)" >> /var/log/shared/date.log;
          sleep 1;
        done
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/shared

  - name: mc-pod-3
    image: busybox:1
    command: ["sh", "-c"]
    args:
      - while true; do
          cat /var/log/shared/date.log;
          sleep 1;
        done
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/shared

  volumes:
  - name: shared-logs
    emptyDir: {}
```
---

> Q2

* This question needs to be solved on node node01. 
* To access the node using SSH:
* As an administrator, you need to prepare node01 to install kubernetes. 
* One of the steps is installing a container runtime. 
* Install the `cri-docker_0.3.16.3-0.debian.deb` package located in /root and ensure that the cri-docker service is running and enabled to start on boot.

> Acceptance

* Is the package installed successfully on node01?
* Is the service running?
* Is the service enabled?

> Solution

```shell
sudo apt install -y ./cri-docker_0.3.16.3-0.debian.deb
sudo systemctl start cri-docker
sudo systemctl enable cri-docker
sudo systemctl status cri-docker
```

---

> Q3

* Create a service `messaging-service` to expose the messaging application within the cluster on port 6379.


* Use imperative commands.

> Acceptance

* Service: messaging-service
* Port: 6379
* Type: ClusterIp
* Use the right labels

> Sol

```shell
kubectl expose pod messaging --port=6379 --name messaging-service
```

---

> Q4 and Q5

* Create a deployment named `hr-web-app` using the image kodekloud/webapp-color with 2 replicas.
* Expose the `hr-web-app` as a service named `hr-web-app-service`, accessible on port `30082` on the nodes of the cluster.

> Acceptance

* deployment
  * Name: hr-web-app
  * Image: kodekloud/webapp-color
  * Replicas: 2
* service
  * Name: hr-web-app-service
  * Type: NodePort
  * Endpoints: 2
  * Port: 8080
  * NodePort: 30082



> Sol

```shell
k create deploy hr-web-app --image=kodekloud/webapp-color --replicas=2
kubectl expose deployment hr-web-app --type=NodePort --port=8080 --name=hr-web-app-service --dry-run=client -o yaml > hr-web-app-service.yaml
```

---

> Q6

* create a Persistent Volume with the given specification: -
* Volume name: pv-analytics
* Storage: 100Mi
* Access mode: ReadWriteMany
* Host path: /pv/data-analytics

> Acceptance

* Is the volume name set?
* Is the storage capacity set?
* Is the accessMode set?
* Is the hostPath set?

> Sol

```yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  hostPath:
      path: /pv/data-analytics
```
---
> Q7

* Create a Horizontal Pod Autoscaler (HPA) with name `webapp-hpa`for the deployment named `kkapp-deploy` in the `default` namespace 
* Ensure that the HPA scales the deployment based on CPU utilization, 
* maintaining an average CPU usage of 50% across all pods.
* Configure the HPA to cautiously scale down pods by setting a stabilization window of 300 seconds to prevent rapid fluctuations in pod count.

* Note: The `kkapp-deploy` deployment is created for backend; you can check in the terminal.

> Acceptance 

* Is the HPA webapp-hpa deployed?
* Is the deployment configured for metrics CPU Utilization?
* Is the stabilization window set to 300 seconds?

> Sol

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300

```

---

> Q8

* Deploy a Vertical Pod Autoscaler (VPA) with name `analytics-vpa` for the deployment named `analytics-deployment` in the `default` namespace.
* The VPA should automatically adjust the CPU and memory requests of the pods to optimize resource utilization. 
* Ensure that the VPA operates in Auto mode, allowing it to evict and recreate pods with updated resource requests as needed.

> Acceptance 

* Is the VPA analytics-vpa created for deployment analytics-deployment?
* Is the updatePolicy set to Auto mode for deployment analytics-deployment?

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"   # ðŸ”¹ ensures pods can be evicted and recreated
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        controlledResources: ["cpu", "memory"]   # ðŸ”¹ optimize CPU + Memory

```

---

> Q9

* create a Kubernetes Gateway resource with the following specifications:
  * Name: web-gateway
  * Namespace: nginx-gateway
  * Gateway Class Name: nginx
  * Listeners:
  * Protocol: HTTP
  * Port: 80
  * Name: http

> Acceptance  

* Is the web-gateway deployed to listen on port 80

> Sol

```yaml
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      protocol: HTTP
      port: 80
```
---

> Q10

One co-worker deployed an nginx helm chart kk-mock1 in the kk-ns namespace on the cluster. 
A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.
After updating the helm chart, upgrade the helm chart version to 18.1.15.

> Acceptance  

Is the deployment running?
Is the chart version upgraded?

> Sol

```shell
helm ls -A
helm repo update kk-mock1 -n kk-ns
helm search repo kk-mock1/nginx -n kk-ns -l | head -n30
helm upgrade kk-mock1 kk-mock1/nginx -n kk-ns --version=18.1.15 
helm ls -n kk-ns

```
---